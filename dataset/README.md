# Phase 1: Dataset Preparation & CNN Model Development (FP32 & FP16)

## 1. Overview

**Key Objectives Achieved in this initial step:**
*   Successful loading and preprocessing of the "PAIN" dataset.
*   Fine-tuning of ResNet50 and SqueezeNet models using transfer learning, meeting or exceeding specified accuracy targets.
*   Conversion of the fine-tuned FP32 models to FP16 precision.
*   Export of all four models (ResNet50-FP32, ResNet50-FP16, SqueezeNet-FP32, SqueezeNet-FP16) to the ONNX format.
*   Logging of training/test accuracies and model file sizes.
*   Generation of non-secure inference output logs for a fixed set of sample images.

## 2. Dataset & Preprocessing

Details regarding the "PAIN" dataset, its initial class distribution, the 80-20 train/test split strategy, data augmentation techniques, and methods for handling class imbalance (WeightedRandomSampler) are documented in:
*   **`dataset_preparation_notes.md`**

The specific PyTorch transforms and transfer learning strategies for each model are also covered in this document. The practical implementation can be seen in Cells 1-5 of `notebook.ipynb`.

## 3. Model Training and Quantization

### 3.0. Pretrained models
- **ResNet-50**  
  - **Pretrained weights**: ImageNet weights from the PyTorch Model Zoo (`torchvision.models.ResNet50_Weights.IMAGENET1K_V2`)  
    (https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html)  
  â€¦

- **SqueezeNet-1.1**  
  - **Pretrained weights**: ImageNet weights via `torchvision.models.squeezenet1_1(pretrained=True)`  
    (https://docs.pytorch.org/vision/main/models/generated/torchvision.models.squeezenet1_1.html)

### 3.1. Fine-tuning (FP32 Models)

ResNet50 and SqueezeNet (version 1.1) models, pre-trained on ImageNet, were fine-tuned on the "PAIN" dataset.

*   **ResNet50:**
    *   Target Accuracy: >80%
    *   Achieved PyTorch Validation Accuracy (FP32): 86.79%
*   **SqueezeNet:**
    *   Target Accuracy: ~60%
    *   Achieved PyTorch Validation Accuracy (FP32): 80.94%

Detailed training logs, including per-epoch accuracy and loss, are available in the output of Cells 4 (ResNet50) and 5 (SqueezeNet) of `notebook.ipynb`.

### 3.2. Quantization (FP16 Models)

Post-training quantization was applied to convert the fine-tuned FP32 models to FP16 precision during the ONNX export process. This was achieved by converting the model and a dummy input to half-precision (`.half()`) before `torch.onnx.export`.

The impact on accuracy was minimal, as detailed in the performance summary below and in the ONNX evaluation section of `notebook.ipynb` (Cell 8 output).

### 3.3. Model Export

All four models were exported to the ONNX (Open Neural Network Exchange) format, with an opset version of 17. This format was chosen for its compatibility with the Athos compiler, which will be used in next steps.

*   ResNet50 (FP32): `resnet50_fp32.onnx`
*   ResNet50 (FP16): `resnet50_fp16.onnx`
*   SqueezeNet (FP32): `sqnet_fp32.onnx`
*   SqueezeNet (FP16): `sqnet_fp16.onnx`

The ONNX export process is detailed in Cell 7 of `notebook.ipynb`. The paths listed in the notebook point to where these models were saved on Google Drive during development. 
To download this models you can use `gdown` with the following ids:
1. `resnet50_fp32.onnx` : `1-0praCGr3SscplXnJVXiWcP0U_11y5dS`
2. `resnet50_fp16.onnx` : `1-4_a5bSOWLzRJTaJy35q5Et_hw-q_YaG`
3. `sqnet_fp32.onnx` : `1-4_a5bSOWLzRJTaJy35q5Et_hw-q_YaG`
4. `sqnet_fp16.onnx`: `1-4mXgMsGUlqI9pl74BHTfpdsXh4WXohv`

## 4. Performance Summary

A consolidated summary of model accuracies (PyTorch training/validation and ONNX test accuracies) and their respective ONNX file sizes is provided in:
*   **`accuracy_and_model_size.csv`**

Key takeaways from the performance summary:
*   Both ResNet50 and SqueezeNet models achieved their target accuracies.
*   The conversion from PyTorch to ONNX FP32 resulted in no loss of accuracy on the test set.
*   FP16 quantization led to a very minor decrease in test accuracy for both architectures while significantly reducing model file sizes (approximately 50% reduction).

## 5. Non-Secure Inference Verification

To establish a baseline for future secure inference verification, inference was performed using a fixed set of sample images (2 per class) on all four model variants (PyTorch FP32, ONNX FP32, ONNX FP16 for both ResNet50 and SqueezeNet). The predicted class, probabilities, and raw logits for each sample image and model were logged.

The detailed outputs are available in:
*   **`phase1_non_secure_sample_inference.txt`** (Generated by Cell 10 of `notebook.ipynb`)

These logs confirm:
*   Close agreement between PyTorch FP32 and ONNX FP32 model outputs.
*   Close agreement between ONNX FP32 and ONNX FP16 model outputs, with minor differences attributable to precision changes.

## 6. Replication and Environment

The `notebook.ipynb` contains all the code used for this phase. To replicate these results:
1.  **Dataset:**
    *   The "PAIN" dataset (zip file) is expected to be available. In the notebook, it's copied from a Google Drive path (`/content/drive/MyDrive/Dataset/FaceClasses.zip`) to the local Colab environment using `!cp` and then unzipped. Users replicating this would need to adjust this path or ensure the dataset is present at `./face_data.zip` before running the relevant cell. The original dataset source is `https://drive.google.com/drive/folders/1RARwImoZgenV6Kdl_jiLsUK01HV61gkA?usp=drive_link`.
2.  **Paths:**
    *   The notebook saves trained PyTorch models (`.pth`) and exported ONNX models (`.onnx`) to a Google Drive directory (`/content/drive/MyDrive/Secure-Inference/models/`). It is easy to modify these paths in the notebook if you wish to save outputs to a different location.
