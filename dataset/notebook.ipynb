{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erw4_RqzKp9P",
        "outputId": "b843c4bf-edff-4c28-d092-034eac0208d0"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKbqeapoKuWC",
        "outputId": "4c839c8a-191c-40a0-9234-9c77bf8c0349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "face_data\n",
            "face_data/FaceClasses\n",
            "face_data/FaceClasses/1\n",
            "face_data/FaceClasses/3\n",
            "face_data/FaceClasses/0\n",
            "face_data/FaceClasses/2\n"
          ]
        }
      ],
      "source": [
        "!cp /content/drive/MyDrive/Dataset/FaceClasses.zip ./face_data.zip\n",
        "!unzip -q face_data.zip -d face_data\n",
        "!find face_data -maxdepth 2 -type d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRMvtnMPKzbz",
        "outputId": "64c1f018-9fca-424d-81eb-d797d5cfd736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class 0: 3092 images\n",
            "Class 1: 2909 images\n",
            "Class 2: 2351 images\n",
            "Class 3: 3109 images\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "class_root = \"face_data/FaceClasses\"\n",
        "\n",
        "\n",
        "for class_name in sorted(os.listdir(class_root)):\n",
        "    class_path = os.path.join(class_root, class_name)\n",
        "    if os.path.isdir(class_path):\n",
        "        count = len([f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))])\n",
        "        print(f\"Class {class_name}: {count} images\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CRPZtmsK4ix",
        "outputId": "48186239-fb0e-4594-ea1c-606fdd70adff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class counts: Counter({3: 2487, 0: 2473, 1: 2327, 2: 1881})\n",
            "Per-class sampling weights: {3: 0.0004020908725371934, 1: 0.0004297378599054577, 2: 0.000531632110579479, 0: 0.0004043671653861706}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Train 1/25: 100%|██████████| 287/287 [01:33<00:00,  3.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 01  TrainLoss 1.2516  TrainAcc 46.58%  ValAcc 55.91%\n",
            " -> New model (ValAcc=55.91%), saved to /content/drive/MyDrive/Secure-Inference/models/resnet50_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 2/25: 100%|██████████| 287/287 [01:26<00:00,  3.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 02  TrainLoss 0.9714  TrainAcc 59.98%  ValAcc 67.38%\n",
            " -> New model (ValAcc=67.38%), saved to /content/drive/MyDrive/Secure-Inference/models/resnet50_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 3/25: 100%|██████████| 287/287 [01:23<00:00,  3.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 03  TrainLoss 0.8218  TrainAcc 67.38%  ValAcc 72.18%\n",
            " -> New model (ValAcc=72.18%), saved to /content/drive/MyDrive/Secure-Inference/models/resnet50_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 4/25: 100%|██████████| 287/287 [01:24<00:00,  3.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 04  TrainLoss 0.7114  TrainAcc 72.80%  ValAcc 74.05%\n",
            " -> New model (ValAcc=74.05%), saved to /content/drive/MyDrive/Secure-Inference/models/resnet50_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 5/25: 100%|██████████| 287/287 [01:24<00:00,  3.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 05  TrainLoss 0.6362  TrainAcc 75.47%  ValAcc 76.62%\n",
            " -> New model (ValAcc=76.62%), saved to /content/drive/MyDrive/Secure-Inference/models/resnet50_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 6/25: 100%|██████████| 287/287 [01:25<00:00,  3.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 06  TrainLoss 0.5950  TrainAcc 77.53%  ValAcc 78.41%\n",
            " -> New model (ValAcc=78.41%), saved to /content/drive/MyDrive/Secure-Inference/models/resnet50_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 7/25: 100%|██████████| 287/287 [01:24<00:00,  3.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 07  TrainLoss 0.5216  TrainAcc 80.14%  ValAcc 80.07%\n",
            " -> New model (ValAcc=80.07%), saved to /content/drive/MyDrive/Secure-Inference/models/resnet50_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 8/25: 100%|██████████| 287/287 [01:25<00:00,  3.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 08  TrainLoss 0.4868  TrainAcc 81.28%  ValAcc 81.86%\n",
            " -> New model (ValAcc=81.86%), saved to /content/drive/MyDrive/Secure-Inference/models/resnet50_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 9/25: 100%|██████████| 287/287 [01:25<00:00,  3.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 09  TrainLoss 0.4309  TrainAcc 83.42%  ValAcc 82.25%\n",
            " -> New model (ValAcc=82.25%), saved to /content/drive/MyDrive/Secure-Inference/models/resnet50_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 10/25: 100%|██████████| 287/287 [01:24<00:00,  3.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10  TrainLoss 0.4208  TrainAcc 84.15%  ValAcc 83.21%\n",
            " -> New model (ValAcc=83.21%), saved to /content/drive/MyDrive/Secure-Inference/models/resnet50_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 11/25: 100%|██████████| 287/287 [01:25<00:00,  3.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 11  TrainLoss 0.4105  TrainAcc 84.93%  ValAcc 83.34%\n",
            " -> New model (ValAcc=83.34%), saved to /content/drive/MyDrive/Secure-Inference/models/resnet50_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 12/25: 100%|██████████| 287/287 [01:25<00:00,  3.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 12  TrainLoss 0.3889  TrainAcc 85.41%  ValAcc 85.04%\n",
            " -> New model (ValAcc=85.04%), saved to /content/drive/MyDrive/Secure-Inference/models/resnet50_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 13/25: 100%|██████████| 287/287 [01:25<00:00,  3.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 13  TrainLoss 0.3734  TrainAcc 85.92%  ValAcc 84.43%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 14/25: 100%|██████████| 287/287 [01:23<00:00,  3.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 14  TrainLoss 0.3535  TrainAcc 86.45%  ValAcc 85.17%\n",
            " -> New model (ValAcc=85.17%), saved to /content/drive/MyDrive/Secure-Inference/models/resnet50_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 15/25: 100%|██████████| 287/287 [01:25<00:00,  3.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 15  TrainLoss 0.3512  TrainAcc 86.57%  ValAcc 85.70%\n",
            " -> New model (ValAcc=85.70%), saved to /content/drive/MyDrive/Secure-Inference/models/resnet50_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 16/25: 100%|██████████| 287/287 [01:24<00:00,  3.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 16  TrainLoss 0.3416  TrainAcc 87.13%  ValAcc 85.56%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 17/25: 100%|██████████| 287/287 [01:24<00:00,  3.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 17  TrainLoss 0.3220  TrainAcc 87.75%  ValAcc 85.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 18/25: 100%|██████████| 287/287 [01:25<00:00,  3.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 18  TrainLoss 0.3335  TrainAcc 87.81%  ValAcc 86.57%\n",
            " -> New model (ValAcc=86.57%), saved to /content/drive/MyDrive/Secure-Inference/models/resnet50_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 19/25: 100%|██████████| 287/287 [01:27<00:00,  3.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 19  TrainLoss 0.3063  TrainAcc 88.50%  ValAcc 86.52%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 20/25: 100%|██████████| 287/287 [01:23<00:00,  3.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 20  TrainLoss 0.3100  TrainAcc 88.36%  ValAcc 85.70%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 21/25: 100%|██████████| 287/287 [01:22<00:00,  3.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 21  TrainLoss 0.3085  TrainAcc 88.75%  ValAcc 85.91%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 22/25: 100%|██████████| 287/287 [01:21<00:00,  3.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 22  TrainLoss 0.2988  TrainAcc 89.23%  ValAcc 86.13%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 23/25: 100%|██████████| 287/287 [01:24<00:00,  3.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 23  TrainLoss 0.3078  TrainAcc 87.85%  ValAcc 86.79%\n",
            " -> New model (ValAcc=86.79%), saved to /content/drive/MyDrive/Secure-Inference/models/resnet50_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 24/25: 100%|██████████| 287/287 [01:25<00:00,  3.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 24  TrainLoss 0.3006  TrainAcc 89.09%  ValAcc 86.61%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 25/25: 100%|██████████| 287/287 [01:23<00:00,  3.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 25  TrainLoss 0.2984  TrainAcc 88.60%  ValAcc 86.22%\n",
            "Training complete. Best Val Acc: 86.79%. Corresponding Train Acc: 87.85%\n"
          ]
        }
      ],
      "source": [
        "import os, random\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.models import ResNet50_Weights\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ─── CONFIG ────────────────────────────────────────────────────────────────\n",
        "DATA_ROOT         = \"face_data/FaceClasses\"\n",
        "BATCH_SIZE        = 32\n",
        "NUM_EPOCHS        = 25\n",
        "LR_HEAD           = 1e-4\n",
        "LR_BACKBONE       = 1e-5\n",
        "WEIGHT_DECAY      = 1e-4\n",
        "DEVICE            = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "RANDOM_SEED       = 42\n",
        "NUM_CLASSES       = len(os.listdir(DATA_ROOT))\n",
        "BEST_MODEL_PATH   = \"/content/drive/MyDrive/Secure-Inference/models/resnet50_mod.pth\"\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# ─── TRANSFORMS ────────────────────────────────────────────────────────────\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.6,1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "# ─── LOAD & SPLIT ──────────────────────────────────────────────────────────\n",
        "full = datasets.ImageFolder(DATA_ROOT)\n",
        "paths, labels = zip(*full.samples)\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    paths, labels, test_size=0.2, stratify=labels, random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "class SubsetFolder(datasets.ImageFolder):\n",
        "    def __init__(self, root, file_list, transform):\n",
        "        super().__init__(root, transform=transform)\n",
        "        self.samples = [(p, self.class_to_idx[Path(p).parent.name]) for p in file_list]\n",
        "        self.targets = [lbl for _, lbl in self.samples]\n",
        "\n",
        "train_ds = SubsetFolder(DATA_ROOT, train_paths, train_tf)\n",
        "val_ds   = SubsetFolder(DATA_ROOT, val_paths,   val_tf)\n",
        "\n",
        "# ─── BALANCED SAMPLER ──────────────────────────────────────────────────────\n",
        "counts = Counter(train_ds.targets)\n",
        "print(\"Class counts:\", counts)\n",
        "class_weights = {cls: 1.0/count for cls, count in counts.items()}\n",
        "sample_weights = [class_weights[t] for t in train_ds.targets]\n",
        "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "print(\"Per-class sampling weights:\", class_weights)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "# ─── MODEL SETUP ──────────────────────────────────────────────────────────\n",
        "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "\n",
        "# 1) Freeze everything\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# 2) Unfreeze last two bottleneck blocks (layer3 & layer4) and layer2\n",
        "for name, p in model.named_parameters():\n",
        "    if name.startswith(('layer2', 'layer3', 'layer4')):\n",
        "        p.requires_grad = True\n",
        "\n",
        "# 3) Replace the head\n",
        "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# ─── LOSS, OPTIMIZER, SCHEDULER ───────────────────────────────────────────\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "opt_params = [\n",
        "    {'params': model.fc.parameters(),      'lr': LR_HEAD},\n",
        "    {'params': [p for n,p in model.named_parameters() if p.requires_grad and not n.startswith('fc')],\n",
        "     'lr': LR_BACKBONE},\n",
        "]\n",
        "optimizer = optim.Adam(opt_params, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
        "\n",
        "# ─── TRAIN / EVAL LOOP ────────────────────────────────────────────────────\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    # — Train —\n",
        "    model.train()\n",
        "    running_loss = running_corrects = 0\n",
        "    for x, y in tqdm(train_loader, desc=f\"Train {epoch}/{NUM_EPOCHS}\"):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "        running_corrects += (out.argmax(1) == y).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "    train_loss = running_loss / len(train_ds)\n",
        "    train_acc  = running_corrects / len(train_ds) * 100\n",
        "\n",
        "    # — Validate —\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            val_corrects += (model(x).argmax(1) == y).sum().item()\n",
        "    val_acc = val_corrects / len(val_ds) * 100\n",
        "\n",
        "    print(f\"\\nEpoch {epoch:02d}  TrainLoss {train_loss:.4f}  TrainAcc {train_acc:.2f}%  ValAcc {val_acc:.2f}%\")\n",
        "\n",
        "    # — Save best —\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_train_acc_for_resnet = train_acc\n",
        "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "        print(f\" -> New model (ValAcc={val_acc:.2f}%), saved to {BEST_MODEL_PATH}\\n\")\n",
        "\n",
        "print(f\"Training complete. Best Val Acc: {best_val_acc:.2f}%. Corresponding Train Acc: {best_train_acc_for_resnet:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oljOrhtK-aB",
        "outputId": "0b49116d-509f-4e66-9858-590dc6f7e9bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/squeezenet1_1-b8a52dc0.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_1-b8a52dc0.pth\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class counts: Counter({3: 2487, 0: 2473, 1: 2327, 2: 1881})\n",
            "Per-class sampling weights: {3: 0.0004020908725371934, 1: 0.0004297378599054577, 2: 0.000531632110579479, 0: 0.0004043671653861706}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.73M/4.73M [00:00<00:00, 64.1MB/s]\n",
            "Train 1/25: 100%|██████████| 287/287 [01:09<00:00,  4.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 01  TrainLoss 1.0779  TrainAcc 55.26%  ValAcc 62.36%\n",
            " → New model (ValAcc=62.36%), saved to /content/drive/MyDrive/Secure-Inference/models/squeezenet_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 2/25: 100%|██████████| 287/287 [01:09<00:00,  4.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 02  TrainLoss 0.9246  TrainAcc 62.79%  ValAcc 67.51%\n",
            " → New model (ValAcc=67.51%), saved to /content/drive/MyDrive/Secure-Inference/models/squeezenet_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 3/25: 100%|██████████| 287/287 [01:10<00:00,  4.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 03  TrainLoss 0.8407  TrainAcc 67.34%  ValAcc 70.30%\n",
            " → New model (ValAcc=70.30%), saved to /content/drive/MyDrive/Secure-Inference/models/squeezenet_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 4/25: 100%|██████████| 287/287 [01:10<00:00,  4.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 04  TrainLoss 0.7886  TrainAcc 69.18%  ValAcc 70.61%\n",
            " → New model (ValAcc=70.61%), saved to /content/drive/MyDrive/Secure-Inference/models/squeezenet_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 5/25: 100%|██████████| 287/287 [01:10<00:00,  4.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 05  TrainLoss 0.7344  TrainAcc 71.73%  ValAcc 71.78%\n",
            " → New model (ValAcc=71.78%), saved to /content/drive/MyDrive/Secure-Inference/models/squeezenet_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 6/25: 100%|██████████| 287/287 [01:10<00:00,  4.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 06  TrainLoss 0.7078  TrainAcc 72.69%  ValAcc 70.91%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 7/25: 100%|██████████| 287/287 [01:08<00:00,  4.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 07  TrainLoss 0.6817  TrainAcc 73.34%  ValAcc 76.93%\n",
            " → New model (ValAcc=76.93%), saved to /content/drive/MyDrive/Secure-Inference/models/squeezenet_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 8/25: 100%|██████████| 287/287 [01:11<00:00,  4.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 08  TrainLoss 0.6624  TrainAcc 74.64%  ValAcc 76.84%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 9/25: 100%|██████████| 287/287 [01:09<00:00,  4.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 09  TrainLoss 0.6175  TrainAcc 76.55%  ValAcc 77.58%\n",
            " → New model (ValAcc=77.58%), saved to /content/drive/MyDrive/Secure-Inference/models/squeezenet_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 10/25: 100%|██████████| 287/287 [01:08<00:00,  4.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10  TrainLoss 0.6085  TrainAcc 76.55%  ValAcc 77.02%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 11/25: 100%|██████████| 287/287 [01:08<00:00,  4.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 11  TrainLoss 0.5966  TrainAcc 76.90%  ValAcc 78.54%\n",
            " → New model (ValAcc=78.54%), saved to /content/drive/MyDrive/Secure-Inference/models/squeezenet_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 12/25: 100%|██████████| 287/287 [01:08<00:00,  4.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 12  TrainLoss 0.5708  TrainAcc 78.22%  ValAcc 77.76%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 13/25: 100%|██████████| 287/287 [01:09<00:00,  4.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 13  TrainLoss 0.5500  TrainAcc 78.95%  ValAcc 79.20%\n",
            " → New model (ValAcc=79.20%), saved to /content/drive/MyDrive/Secure-Inference/models/squeezenet_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 14/25: 100%|██████████| 287/287 [01:09<00:00,  4.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 14  TrainLoss 0.5424  TrainAcc 79.22%  ValAcc 79.59%\n",
            " → New model (ValAcc=79.59%), saved to /content/drive/MyDrive/Secure-Inference/models/squeezenet_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 15/25: 100%|██████████| 287/287 [01:11<00:00,  4.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 15  TrainLoss 0.5267  TrainAcc 80.24%  ValAcc 79.81%\n",
            " → New model (ValAcc=79.81%), saved to /content/drive/MyDrive/Secure-Inference/models/squeezenet_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 16/25: 100%|██████████| 287/287 [01:09<00:00,  4.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 16  TrainLoss 0.5244  TrainAcc 79.43%  ValAcc 80.68%\n",
            " → New model (ValAcc=80.68%), saved to /content/drive/MyDrive/Secure-Inference/models/squeezenet_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 17/25: 100%|██████████| 287/287 [01:08<00:00,  4.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 17  TrainLoss 0.5185  TrainAcc 80.29%  ValAcc 79.85%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 18/25: 100%|██████████| 287/287 [01:08<00:00,  4.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 18  TrainLoss 0.5221  TrainAcc 79.85%  ValAcc 79.68%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 19/25: 100%|██████████| 287/287 [01:08<00:00,  4.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 19  TrainLoss 0.5019  TrainAcc 80.89%  ValAcc 80.90%\n",
            " → New model (ValAcc=80.90%), saved to /content/drive/MyDrive/Secure-Inference/models/squeezenet_mod.pth\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 20/25: 100%|██████████| 287/287 [01:09<00:00,  4.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 20  TrainLoss 0.4889  TrainAcc 81.21%  ValAcc 80.59%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 21/25: 100%|██████████| 287/287 [01:09<00:00,  4.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 21  TrainLoss 0.5023  TrainAcc 80.88%  ValAcc 80.33%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 22/25: 100%|██████████| 287/287 [01:09<00:00,  4.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 22  TrainLoss 0.5081  TrainAcc 81.30%  ValAcc 80.51%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 23/25: 100%|██████████| 287/287 [01:09<00:00,  4.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 23  TrainLoss 0.4919  TrainAcc 81.52%  ValAcc 80.68%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 24/25: 100%|██████████| 287/287 [01:09<00:00,  4.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 24  TrainLoss 0.4912  TrainAcc 81.54%  ValAcc 80.85%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train 25/25: 100%|██████████| 287/287 [01:09<00:00,  4.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 25  TrainLoss 0.4968  TrainAcc 81.16%  ValAcc 80.94%\n",
            " → New model (ValAcc=80.94%), saved to /content/drive/MyDrive/Secure-Inference/models/squeezenet_mod.pth\n",
            "\n",
            "Training complete. Best Val Acc: 80.94%. Corresponding Train Acc: 81.16%\n"
          ]
        }
      ],
      "source": [
        "import os, random\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from torchvision import datasets, models, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ─── CONFIG ────────────────────────────────────────────────────────────────\n",
        "DATA_ROOT       = \"face_data/FaceClasses\"\n",
        "BATCH_SIZE      = 32\n",
        "NUM_EPOCHS      = 25\n",
        "LR_HEAD         = 1e-3\n",
        "LR_BACKBONE     = 1e-4\n",
        "WEIGHT_DECAY    = 1e-4\n",
        "DEVICE          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "RANDOM_SEED     = 42\n",
        "NUM_CLASSES     = len(os.listdir(DATA_ROOT))\n",
        "BEST_MODEL_PATH = \"/content/drive/MyDrive/Secure-Inference/models/squeezenet_mod.pth\"\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# ─── TRANSFORMS ────────────────────────────────────────────────────────────\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.6,1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "# ─── LOAD & SPLIT ──────────────────────────────────────────────────────────\n",
        "full = datasets.ImageFolder(DATA_ROOT)\n",
        "paths, labels = zip(*full.samples)\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    paths, labels, test_size=0.2, stratify=labels, random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "class SubsetFolder(datasets.ImageFolder):\n",
        "    def __init__(self, root, file_list, transform):\n",
        "        super().__init__(root, transform=transform)\n",
        "        self.samples = [(p, self.class_to_idx[Path(p).parent.name]) for p in file_list]\n",
        "        self.targets = [lbl for _, lbl in self.samples]\n",
        "\n",
        "train_ds = SubsetFolder(DATA_ROOT, train_paths, train_tf)\n",
        "val_ds   = SubsetFolder(DATA_ROOT, val_paths,   val_tf)\n",
        "\n",
        "# ─── SAMPLER ───────────────────────────────────────────────────────────────\n",
        "counts = Counter(train_ds.targets)\n",
        "print(\"Class counts:\", counts)\n",
        "class_weights = {cls: 1.0/count for cls, count in counts.items()}\n",
        "sample_weights = [class_weights[t] for t in train_ds.targets]\n",
        "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "print(\"Per-class sampling weights:\", class_weights)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "# ─── MODEL SETUP ──────────────────────────────────────────────────────────\n",
        "model = models.squeezenet1_1(pretrained=True)\n",
        "\n",
        "# Freeze all\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Unfreeze Fire modules 7 & 8 for more capacity\n",
        "for name, p in model.named_parameters():\n",
        "    if name.startswith((\"features.12\", \"features.14\")):  # fire7 & fire8\n",
        "        p.requires_grad = True\n",
        "\n",
        "# Replace classifier\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Conv2d(512, NUM_CLASSES, kernel_size=1),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.AdaptiveAvgPool2d((1,1))\n",
        ")\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# ─── LOSS, OPT, SCHED ──────────────────────────────────────────────────────\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "opt_params = [\n",
        "    {'params': model.classifier.parameters(), 'lr': LR_HEAD},\n",
        "    {'params': [p for n,p in model.named_parameters() if p.requires_grad and \"classifier\" not in n],\n",
        "     'lr': LR_BACKBONE},\n",
        "]\n",
        "optimizer = optim.Adam(opt_params, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
        "\n",
        "# ─── TRAIN & VALIDATE ──────────────────────────────────────────────────────\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    model.train()\n",
        "    run_loss = run_corr = 0\n",
        "    for x,y in tqdm(train_loader, desc=f\"Train {epoch}/{NUM_EPOCHS}\"):\n",
        "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        out = out.view(out.size(0), NUM_CLASSES)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        run_loss += loss.item() * x.size(0)\n",
        "        run_corr += (out.argmax(1)==y).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "    train_loss = run_loss/len(train_ds)\n",
        "    train_acc  = run_corr/len(train_ds)*100\n",
        "\n",
        "    model.eval()\n",
        "    val_corr = 0\n",
        "    with torch.no_grad():\n",
        "        for x,y in val_loader:\n",
        "            x,y = x.to(DEVICE), y.to(DEVICE)\n",
        "            out = model(x).view(x.size(0), NUM_CLASSES)\n",
        "            val_corr += (out.argmax(1)==y).sum().item()\n",
        "    val_acc = val_corr/len(val_ds)*100\n",
        "\n",
        "    print(f\"\\nEpoch {epoch:02d}  TrainLoss {train_loss:.4f}  TrainAcc {train_acc:.2f}%  ValAcc {val_acc:.2f}%\")\n",
        "    if val_acc>best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_train_acc_for_sqnet = train_acc\n",
        "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "        print(f\" → New model (ValAcc={val_acc:.2f}%), saved to {BEST_MODEL_PATH}\\n\")\n",
        "\n",
        "print(f\"Training complete. Best Val Acc: {best_val_acc:.2f}%. Corresponding Train Acc: {best_train_acc_for_sqnet:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIuQKF3gLFOp",
        "outputId": "bd6cc9e6-e7b3-4c19-e181-b5f0e315dce4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting onnxruntime-gpu\n",
            "  Downloading onnxruntime_gpu-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.4)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.13.2)\n",
            "Collecting coloredlogs (from onnxruntime-gpu)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime_gpu-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (283.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, humanfriendly, coloredlogs, onnxruntime-gpu\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.18.0 onnxruntime-gpu-1.22.0\n"
          ]
        }
      ],
      "source": [
        "!pip install onnx onnxruntime-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33W21NR7LTHB",
        "outputId": "1d9ad58c-e181-40ef-e991-8b0207ffe4a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exported /content/drive/MyDrive/Secure-Inference/models/resnet50_fp32.onnx (89.6 MB)\n",
            "Exported /content/drive/MyDrive/Secure-Inference/models/resnet50_fp16.onnx (44.8 MB)\n",
            "Exported /content/drive/MyDrive/Secure-Inference/models/sqnet_fp32.onnx (2.8 MB)\n",
            "Exported /content/drive/MyDrive/Secure-Inference/models/sqnet_fp16.onnx (1.4 MB)\n"
          ]
        }
      ],
      "source": [
        "import torch, os\n",
        "\n",
        "def export_onnx(model, dummy_input, path, opset=17, half=False):\n",
        "    if half:\n",
        "        model = model.half()\n",
        "        dummy_input = dummy_input.half()\n",
        "    torch.onnx.export(\n",
        "        model, dummy_input, path,\n",
        "        input_names=[\"input\"], output_names=[\"output\"],\n",
        "        opset_version=opset,\n",
        "        dynamic_axes = None,\n",
        "    )\n",
        "    size_mb = os.path.getsize(path) / (1024*1024)\n",
        "    print(f\"Exported {path} ({size_mb:.1f} MB)\")\n",
        "\n",
        "# ───── Load Models ──────────────────────────────────────────────────\n",
        "device = \"cpu\"\n",
        "\n",
        "# ResNet50\n",
        "resnet = models.resnet50(weights=None, num_classes=NUM_CLASSES)\n",
        "resnet.load_state_dict(torch.load(\"/content/drive/MyDrive/Secure-Inference/models/resnet50_mod.pth\", map_location=device))\n",
        "resnet.eval()\n",
        "\n",
        "# SqueezeNet\n",
        "sq = models.squeezenet1_1(weights=None)\n",
        "# same classifier override as training\n",
        "sq.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Conv2d(512, NUM_CLASSES, 1),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.AdaptiveAvgPool2d((1,1))\n",
        ")\n",
        "sq.load_state_dict(torch.load(\"/content/drive/MyDrive/Secure-Inference/models/squeezenet_mod.pth\", map_location=device))\n",
        "sq.eval()\n",
        "\n",
        "# ───── Dummy Input ───────────────────────────────────────────────────────────\n",
        "dummy = torch.randn(1,3,224,224, device=device)\n",
        "\n",
        "# 1. ResNet50 FP32\n",
        "export_onnx(resnet, dummy,    \"/content/drive/MyDrive/Secure-Inference/models/resnet50_fp32.onnx\", half=False)\n",
        "\n",
        "# 2. ResNet50 FP16\n",
        "export_onnx(resnet, dummy,    \"/content/drive/MyDrive/Secure-Inference/models/resnet50_fp16.onnx\", half=True)\n",
        "\n",
        "# 3. SqueezeNet FP32\n",
        "export_onnx(sq,     dummy,    \"/content/drive/MyDrive/Secure-Inference/models/sqnet_fp32.onnx\",   half=False)\n",
        "\n",
        "# 4. SqueezeNet FP16\n",
        "export_onnx(sq,     dummy,    \"/content/drive/MyDrive/Secure-Inference/models/sqnet_fp16.onnx\",   half=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE6rv4tDlrys",
        "outputId": "50b36270-e2d5-404d-a6b7-73ebd8b0221e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ONNX Test Loader created with 2293 samples (using existing val_paths and SubsetFolder).\n",
            "\n",
            "Evaluating ResNet50 FP32...\n",
            "Loading ONNX model resnet50_fp32.onnx with providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "Using ONNX provider: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "resnet50_fp32.onnx: 100%|██████████| 2293/2293 [00:16<00:00, 138.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating ResNet50 FP16...\n",
            "Loading ONNX model resnet50_fp16.onnx with providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "Using ONNX provider: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "resnet50_fp16.onnx: 100%|██████████| 2293/2293 [00:15<00:00, 148.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating SqueezeNet FP32...\n",
            "Loading ONNX model sqnet_fp32.onnx with providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "Using ONNX provider: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sqnet_fp32.onnx: 100%|██████████| 2293/2293 [00:11<00:00, 196.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating SqueezeNet FP16...\n",
            "Loading ONNX model sqnet_fp16.onnx with providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "Using ONNX provider: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sqnet_fp16.onnx: 100%|██████████| 2293/2293 [00:13<00:00, 174.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ONNX Model Accuracies on Test Set ---\n",
            "  ResNet50 FP32        → 86.79%\n",
            "  ResNet50 FP16        → 86.70%\n",
            "  SqueezeNet FP32      → 80.94%\n",
            "  SqueezeNet FP16      → 80.90%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import onnxruntime as rt\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "\n",
        "ONNX_EVAL_BATCH_SIZE = 1\n",
        "\n",
        "onnx_eval_transform = val_tf\n",
        "\n",
        "try:\n",
        "    val_ds_for_onnx = SubsetFolder(root=DATA_ROOT,\n",
        "                                   file_list=val_paths,\n",
        "                                   transform=onnx_eval_transform)\n",
        "\n",
        "    onnx_test_loader = DataLoader(\n",
        "        val_ds_for_onnx,\n",
        "        batch_size=ONNX_EVAL_BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    print(f\"ONNX Test Loader created with {len(val_ds_for_onnx)} samples (using existing val_paths and SubsetFolder).\")\n",
        "except NameError as e:\n",
        "    onnx_test_loader = None\n",
        "\n",
        "\n",
        "def eval_onnx_gpu(model_path, loader):\n",
        "    if loader is None:\n",
        "        print(f\"Skipping evaluation for {os.path.basename(model_path)} due to loader setup issues.\")\n",
        "        return 0.0\n",
        "\n",
        "    providers = rt.get_available_providers()\n",
        "    preferred_providers = []\n",
        "    if 'CUDAExecutionProvider' in providers: preferred_providers.append('CUDAExecutionProvider')\n",
        "    if 'CPUExecutionProvider' in providers: preferred_providers.append('CPUExecutionProvider')\n",
        "    if not preferred_providers: raise RuntimeError(\"No CUDA or CPU ONNX Execution Provider found.\")\n",
        "\n",
        "    print(f\"Loading ONNX model {os.path.basename(model_path)} with providers: {preferred_providers}\")\n",
        "    sess = rt.InferenceSession(model_path, providers=preferred_providers)\n",
        "    print(f\"Using ONNX provider: {sess.get_providers()}\")\n",
        "\n",
        "    inp_meta = sess.get_inputs()[0]\n",
        "    inp_name = inp_meta.name\n",
        "    elem_type_str = rt.OrtTensorType.elem_type_to_string(inp_meta.type.tensor_type.elem_type) if hasattr(inp_meta.type, 'tensor_type') else inp_meta.type\n",
        "    expects_fp16 = \"float16\" in elem_type_str.lower()\n",
        "\n",
        "    correct = total = 0\n",
        "    for images, labels in tqdm(loader, desc=os.path.basename(model_path)):\n",
        "        arr = images.numpy()\n",
        "        if expects_fp16 and arr.dtype != np.float16:\n",
        "            arr = arr.astype(np.float16)\n",
        "        elif not expects_fp16 and arr.dtype != np.float32:\n",
        "            arr = arr.astype(np.float32)\n",
        "\n",
        "        ort_inputs = {inp_name: arr}\n",
        "        logits = sess.run(None, ort_inputs)[0]\n",
        "        preds  = np.argmax(logits, axis=1)\n",
        "        correct += (preds == labels.numpy()).sum()\n",
        "        total   += labels.size(0)\n",
        "\n",
        "    return (correct / total * 100) if total > 0 else 0.0\n",
        "\n",
        "# ONNX Model Paths\n",
        "onnx_model_configs = {\n",
        "    \"ResNet50 FP32\":   \"/content/drive/MyDrive/Secure-Inference/models/resnet50_fp32.onnx\",\n",
        "    \"ResNet50 FP16\":   \"/content/drive/MyDrive/Secure-Inference/models/resnet50_fp16.onnx\",\n",
        "    \"SqueezeNet FP32\": \"/content/drive/MyDrive/Secure-Inference/models/sqnet_fp32.onnx\",\n",
        "    \"SqueezeNet FP16\": \"/content/drive/MyDrive/Secure-Inference/models/sqnet_fp16.onnx\",\n",
        "}\n",
        "\n",
        "onnx_results = {}\n",
        "if onnx_test_loader:\n",
        "    for name, path in onnx_model_configs.items():\n",
        "        print(f\"\\nEvaluating {name}...\")\n",
        "        assert os.path.isfile(path), f\"ONNX Model file missing: {path}\"\n",
        "        onnx_results[name] = eval_onnx_gpu(path, onnx_test_loader)\n",
        "\n",
        "print(\"\\n--- ONNX Model Accuracies on Test Set ---\")\n",
        "for name, acc in onnx_results.items():\n",
        "    print(f\"  {name:20s} → {acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbpLRyWInFIk",
        "outputId": "96d8b050-10ac-410b-c444-25daa9050c92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phase 1 Sample Inference Logging Started. Results will be saved to: /content/drive/MyDrive/Secure-Inference/phase1_results/phase1_non_secure_sample_inference.txt\n",
            "Using device: cuda\n",
            "DATA_ROOT for sample images: face_data/FaceClasses\n",
            "Number of classes (NUM_CLASSES): 4\n",
            "'val_tf' (validation transform) found in scope and will be used.\n",
            "ResNet50 PyTorch model loaded from /content/drive/MyDrive/Secure-Inference/models/resnet50_mod.pth\n",
            "SqueezeNet PyTorch model loaded from /content/drive/MyDrive/Secure-Inference/models/squeezenet_mod.pth\n",
            "ONNX session created for ResNet50_FP32 using ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "ONNX session created for ResNet50_FP16 using ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "ONNX session created for SqueezeNet_FP32 using ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "ONNX session created for SqueezeNet_FP16 using ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "\n",
            "Using FIXED sample images: {'0': ['aa048t2aaunaff005.png', 'aa048t2aaunaff008.png'], '1': ['aa048t2aeaff017.png', 'aa048t2aeaff018.png'], '2': ['ak064t1aaaff100.png', 'ak064t1aaaff101.png'], '3': ['ak064t1aaaff095.png', 'ak064t1aaaff096.png']}\n",
            "Verified sample images to be processed: {'0': ['face_data/FaceClasses/0/aa048t2aaunaff005.png', 'face_data/FaceClasses/0/aa048t2aaunaff008.png'], '1': ['face_data/FaceClasses/1/aa048t2aeaff017.png', 'face_data/FaceClasses/1/aa048t2aeaff018.png'], '2': ['face_data/FaceClasses/2/ak064t1aaaff100.png', 'face_data/FaceClasses/2/ak064t1aaaff101.png'], '3': ['face_data/FaceClasses/3/ak064t1aaaff095.png', 'face_data/FaceClasses/3/ak064t1aaaff096.png']}\n",
            "\n",
            "============================== SAMPLE INFERENCE RESULTS ==============================\n",
            "\n",
            "--- Image: face_data/FaceClasses/0/aa048t2aaunaff005.png (True Label: 0) ---\n",
            "  [PyTorch ResNet50]:\n",
            "    Predicted Class: 0, Probabilities: [9.8963e-01 9.8646e-03 3.7636e-04 1.3234e-04]\n",
            "    Logits: [ 4.5555 -0.0529 -3.3191 -4.3643]\n",
            "  [PyTorch SqueezeNet]:\n",
            "    Predicted Class: 0, Probabilities: [9.6663e-01 3.1075e-02 1.8421e-03 4.5298e-04]\n",
            "    Logits: [11.0535  7.6161  4.7906  3.3878]\n",
            "  [ONNX ResNet50_FP32]:\n",
            "    Predicted Class: 0, Probabilities: [9.8963e-01 9.8646e-03 3.7635e-04 1.3234e-04]\n",
            "    Logits: [ 4.5555 -0.0529 -3.3191 -4.3643]\n",
            "  [ONNX ResNet50_FP16]:\n",
            "    Predicted Class: 0, Probabilities: [9.902e-01 9.560e-03 3.510e-04 1.385e-04]\n",
            "    Logits: [ 4.562  -0.0768 -3.385  -4.316 ]\n",
            "  [ONNX SqueezeNet_FP32]:\n",
            "    Predicted Class: 0, Probabilities: [9.6663e-01 3.1075e-02 1.8421e-03 4.5298e-04]\n",
            "    Logits: [11.0535  7.6161  4.7906  3.3878]\n",
            "  [ONNX SqueezeNet_FP16]:\n",
            "    Predicted Class: 0, Probabilities: [9.668e-01 3.119e-02 1.852e-03 4.520e-04]\n",
            "    Logits: [11.055  7.62   4.797  3.387]\n",
            "\n",
            "--- Image: face_data/FaceClasses/0/aa048t2aaunaff008.png (True Label: 0) ---\n",
            "  [PyTorch ResNet50]:\n",
            "    Predicted Class: 0, Probabilities: [9.8646e-01 1.0293e-02 3.1186e-03 1.3048e-04]\n",
            "    Logits: [ 3.9803 -0.5824 -1.7765 -4.9504]\n",
            "  [PyTorch SqueezeNet]:\n",
            "    Predicted Class: 0, Probabilities: [9.6941e-01 2.9501e-02 9.7796e-04 1.1406e-04]\n",
            "    Logits: [11.842   8.3497  4.943   2.7942]\n",
            "  [ONNX ResNet50_FP32]:\n",
            "    Predicted Class: 0, Probabilities: [9.8646e-01 1.0293e-02 3.1186e-03 1.3048e-04]\n",
            "    Logits: [ 3.9803 -0.5824 -1.7764 -4.9504]\n",
            "  [ONNX ResNet50_FP16]:\n",
            "    Predicted Class: 0, Probabilities: [9.854e-01 1.061e-02 3.378e-03 1.315e-04]\n",
            "    Logits: [ 3.953  -0.5796 -1.724  -4.973 ]\n",
            "  [ONNX SqueezeNet_FP32]:\n",
            "    Predicted Class: 0, Probabilities: [9.6941e-01 2.9501e-02 9.7796e-04 1.1406e-04]\n",
            "    Logits: [11.842   8.3497  4.943   2.7942]\n",
            "  [ONNX SqueezeNet_FP16]:\n",
            "    Predicted Class: 0, Probabilities: [9.688e-01 2.971e-02 9.899e-04 1.141e-04]\n",
            "    Logits: [11.836  8.35   4.95   2.793]\n",
            "\n",
            "--- Image: face_data/FaceClasses/1/aa048t2aeaff017.png (True Label: 1) ---\n",
            "  [PyTorch ResNet50]:\n",
            "    Predicted Class: 1, Probabilities: [5.4442e-04 9.8651e-01 2.5319e-03 1.0418e-02]\n",
            "    Logits: [-3.806   3.6962 -2.269  -0.8545]\n",
            "  [PyTorch SqueezeNet]:\n",
            "    Predicted Class: 1, Probabilities: [0.0155 0.9338 0.0381 0.0127]\n",
            "    Logits: [ 6.1115 10.2124  7.014   5.9113]\n",
            "  [ONNX ResNet50_FP32]:\n",
            "    Predicted Class: 1, Probabilities: [5.4444e-04 9.8651e-01 2.5319e-03 1.0418e-02]\n",
            "    Logits: [-3.806   3.6962 -2.269  -0.8545]\n",
            "  [ONNX ResNet50_FP16]:\n",
            "    Predicted Class: 1, Probabilities: [5.536e-04 9.854e-01 2.769e-03 1.078e-02]\n",
            "    Logits: [-3.818   3.664  -2.21   -0.8516]\n",
            "  [ONNX SqueezeNet_FP32]:\n",
            "    Predicted Class: 1, Probabilities: [0.0155 0.9338 0.0381 0.0127]\n",
            "    Logits: [ 6.1115 10.2124  7.014   5.9113]\n",
            "  [ONNX SqueezeNet_FP16]:\n",
            "    Predicted Class: 1, Probabilities: [0.0153 0.934  0.0381 0.0125]\n",
            "    Logits: [ 6.11  10.22   7.02   5.906]\n",
            "\n",
            "--- Image: face_data/FaceClasses/1/aa048t2aeaff018.png (True Label: 1) ---\n",
            "  [PyTorch ResNet50]:\n",
            "    Predicted Class: 1, Probabilities: [2.2294e-04 9.9708e-01 1.7316e-03 9.6491e-04]\n",
            "    Logits: [-3.8817  4.524  -1.8318 -2.4165]\n",
            "  [PyTorch SqueezeNet]:\n",
            "    Predicted Class: 1, Probabilities: [0.0123 0.9618 0.0203 0.0056]\n",
            "    Logits: [ 6.1784 10.5342  6.6754  5.3896]\n",
            "  [ONNX ResNet50_FP32]:\n",
            "    Predicted Class: 1, Probabilities: [2.2294e-04 9.9708e-01 1.7316e-03 9.6492e-04]\n",
            "    Logits: [-3.8817  4.524  -1.8318 -2.4165]\n",
            "  [ONNX ResNet50_FP16]:\n",
            "    Predicted Class: 1, Probabilities: [2.229e-04 9.971e-01 1.947e-03 1.039e-03]\n",
            "    Logits: [-3.918  4.484 -1.753 -2.383]\n",
            "  [ONNX SqueezeNet_FP32]:\n",
            "    Predicted Class: 1, Probabilities: [0.0123 0.9618 0.0203 0.0056]\n",
            "    Logits: [ 6.1784 10.5342  6.6754  5.3896]\n",
            "  [ONNX SqueezeNet_FP16]:\n",
            "    Predicted Class: 1, Probabilities: [0.0123 0.9614 0.0203 0.0055]\n",
            "    Logits: [ 6.18  10.54   6.68   5.383]\n",
            "\n",
            "--- Image: face_data/FaceClasses/2/ak064t1aaaff100.png (True Label: 2) ---\n",
            "  [PyTorch ResNet50]:\n",
            "    Predicted Class: 0, Probabilities: [0.4645 0.0597 0.4164 0.0594]\n",
            "    Logits: [ 0.445  -1.606   0.3356 -1.6113]\n",
            "  [PyTorch SqueezeNet]:\n",
            "    Predicted Class: 1, Probabilities: [0.273  0.3481 0.1301 0.2488]\n",
            "    Logits: [7.4287 7.6714 6.6877 7.3355]\n",
            "  [ONNX ResNet50_FP32]:\n",
            "    Predicted Class: 0, Probabilities: [0.4645 0.0597 0.4164 0.0594]\n",
            "    Logits: [ 0.445  -1.606   0.3356 -1.6113]\n",
            "  [ONNX ResNet50_FP16]:\n",
            "    Predicted Class: 0, Probabilities: [0.446  0.0583 0.437  0.0587]\n",
            "    Logits: [ 0.3992 -1.635   0.3792 -1.629 ]\n",
            "  [ONNX SqueezeNet_FP32]:\n",
            "    Predicted Class: 1, Probabilities: [0.273  0.3481 0.1301 0.2488]\n",
            "    Logits: [7.4287 7.6714 6.6877 7.3355]\n",
            "  [ONNX SqueezeNet_FP16]:\n",
            "    Predicted Class: 1, Probabilities: [0.2725 0.3499 0.1293 0.2482]\n",
            "    Logits: [7.426 7.676 6.68  7.332]\n",
            "\n",
            "--- Image: face_data/FaceClasses/2/ak064t1aaaff101.png (True Label: 2) ---\n",
            "  [PyTorch ResNet50]:\n",
            "    Predicted Class: 0, Probabilities: [0.678  0.0032 0.2694 0.0494]\n",
            "    Logits: [ 1.4298 -3.9329  0.5069 -1.189 ]\n",
            "  [PyTorch SqueezeNet]:\n",
            "    Predicted Class: 0, Probabilities: [0.6191 0.0891 0.0891 0.2028]\n",
            "    Logits: [8.5286 6.5896 6.5898 7.4126]\n",
            "  [ONNX ResNet50_FP32]:\n",
            "    Predicted Class: 0, Probabilities: [0.678  0.0032 0.2694 0.0494]\n",
            "    Logits: [ 1.4298 -3.9329  0.5069 -1.189 ]\n",
            "  [ONNX ResNet50_FP16]:\n",
            "    Predicted Class: 0, Probabilities: [0.68   0.0025 0.2773 0.0399]\n",
            "    Logits: [ 1.526  -4.082   0.6294 -1.31  ]\n",
            "  [ONNX SqueezeNet_FP32]:\n",
            "    Predicted Class: 0, Probabilities: [0.6191 0.0891 0.0891 0.2028]\n",
            "    Logits: [8.5286 6.5896 6.5898 7.4126]\n",
            "  [ONNX SqueezeNet_FP16]:\n",
            "    Predicted Class: 0, Probabilities: [0.62   0.09   0.0891 0.2007]\n",
            "    Logits: [8.53  6.6   6.59  7.402]\n",
            "\n",
            "--- Image: face_data/FaceClasses/3/ak064t1aaaff095.png (True Label: 3) ---\n",
            "  [PyTorch ResNet50]:\n",
            "    Predicted Class: 3, Probabilities: [0.0583 0.1061 0.0599 0.7757]\n",
            "    Logits: [-1.3443 -0.7448 -1.3173  1.2444]\n",
            "  [PyTorch SqueezeNet]:\n",
            "    Predicted Class: 1, Probabilities: [0.0389 0.7388 0.0299 0.1924]\n",
            "    Logits: [6.0033 8.947  5.7389 7.6016]\n",
            "  [ONNX ResNet50_FP32]:\n",
            "    Predicted Class: 3, Probabilities: [0.0583 0.1061 0.0599 0.7757]\n",
            "    Logits: [-1.3443 -0.7448 -1.3173  1.2444]\n",
            "  [ONNX ResNet50_FP16]:\n",
            "    Predicted Class: 3, Probabilities: [0.0562 0.1031 0.062  0.779 ]\n",
            "    Logits: [-1.373 -0.764 -1.275  1.257]\n",
            "  [ONNX SqueezeNet_FP32]:\n",
            "    Predicted Class: 1, Probabilities: [0.0389 0.7388 0.0299 0.1924]\n",
            "    Logits: [6.0033 8.947  5.7389 7.6016]\n",
            "  [ONNX SqueezeNet_FP16]:\n",
            "    Predicted Class: 1, Probabilities: [0.0389 0.7393 0.0298 0.192 ]\n",
            "    Logits: [6.008 8.95  5.742 7.605]\n",
            "\n",
            "--- Image: face_data/FaceClasses/3/ak064t1aaaff096.png (True Label: 3) ---\n",
            "  [PyTorch ResNet50]:\n",
            "    Predicted Class: 3, Probabilities: [0.1269 0.1293 0.0782 0.6657]\n",
            "    Logits: [-0.8307 -0.8121 -1.3149  0.827 ]\n",
            "  [PyTorch SqueezeNet]:\n",
            "    Predicted Class: 3, Probabilities: [0.012  0.136  0.0212 0.8307]\n",
            "    Logits: [ 5.8619  8.2863  6.4275 10.0958]\n",
            "  [ONNX ResNet50_FP32]:\n",
            "    Predicted Class: 3, Probabilities: [0.1269 0.1293 0.0782 0.6657]\n",
            "    Logits: [-0.8307 -0.8121 -1.3149  0.8269]\n",
            "  [ONNX ResNet50_FP16]:\n",
            "    Predicted Class: 3, Probabilities: [0.1365 0.1387 0.0866 0.638 ]\n",
            "    Logits: [-0.806  -0.791  -1.262   0.7363]\n",
            "  [ONNX SqueezeNet_FP32]:\n",
            "    Predicted Class: 3, Probabilities: [0.012  0.136  0.0212 0.8307]\n",
            "    Logits: [ 5.8619  8.2863  6.4275 10.0958]\n",
            "  [ONNX SqueezeNet_FP16]:\n",
            "    Predicted Class: 3, Probabilities: [0.0121 0.1366 0.0213 0.8296]\n",
            "    Logits: [ 5.863  8.29   6.434 10.09 ]\n",
            "\n",
            "============================== Sample Inference Logging Complete ==============================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms, datasets\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import onnxruntime as rt\n",
        "from pathlib import Path\n",
        "\n",
        "# ─── CONFIGURATIONS ─────────────────────────────────────────────────────────\n",
        "DRIVE_MODEL_DIR = \"/content/drive/MyDrive/Secure-Inference/models/\"\n",
        "DATA_ROOT = \"face_data/FaceClasses\"\n",
        "NUM_CLASSES = 4\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "IMAGES_PER_CLASS_TO_SAMPLE = 2\n",
        "\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/Secure-Inference/phase1_results\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "INFERENCE_LOG_FILE = os.path.join(RESULTS_DIR, \"phase1_non_secure_sample_inference.txt\")\n",
        "\n",
        "\n",
        "if os.path.exists(INFERENCE_LOG_FILE):\n",
        "    os.remove(INFERENCE_LOG_FILE)\n",
        "\n",
        "def log_inference(message):\n",
        "    print(message)\n",
        "    with open(INFERENCE_LOG_FILE, \"a\") as f:\n",
        "        f.write(message + \"\\n\")\n",
        "\n",
        "log_inference(f\"Phase 1 Sample Inference Logging Started. Results will be saved to: {INFERENCE_LOG_FILE}\")\n",
        "log_inference(f\"Using device: {DEVICE}\")\n",
        "log_inference(f\"DATA_ROOT for sample images: {DATA_ROOT}\")\n",
        "log_inference(f\"Number of classes (NUM_CLASSES): {NUM_CLASSES}\")\n",
        "\n",
        "# ─── TRANSFORMS\n",
        "if 'val_tf' not in locals():\n",
        "    log_inference(\"Warning: 'val_tf' not in scope. Redefining standard validation transform.\")\n",
        "    val_tf = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "else:\n",
        "    log_inference(\"'val_tf' (validation transform) found in scope and will be used.\")\n",
        "\n",
        "\n",
        "# ─── LOAD PYTORCH MODELS ───────────────────────────────────────────────────\n",
        "resnet_pytorch = models.resnet50(weights=None, num_classes=NUM_CLASSES)\n",
        "resnet_pytorch.load_state_dict(torch.load(os.path.join(DRIVE_MODEL_DIR, \"resnet50_mod.pth\"), map_location=DEVICE))\n",
        "resnet_pytorch.eval().to(DEVICE)\n",
        "log_inference(f\"ResNet50 PyTorch model loaded from {os.path.join(DRIVE_MODEL_DIR, 'resnet50_mod.pth')}\")\n",
        "\n",
        "# SqueezeNet\n",
        "sqnet_pytorch = models.squeezenet1_1(num_classes=NUM_CLASSES)\n",
        "\n",
        "sqnet_pytorch.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Conv2d(512, NUM_CLASSES, kernel_size=1),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.AdaptiveAvgPool2d((1, 1))\n",
        ")\n",
        "sqnet_pytorch.load_state_dict(torch.load(os.path.join(DRIVE_MODEL_DIR, \"squeezenet_mod.pth\"), map_location=DEVICE))\n",
        "sqnet_pytorch.eval().to(DEVICE)\n",
        "log_inference(f\"SqueezeNet PyTorch model loaded from {os.path.join(DRIVE_MODEL_DIR, 'squeezenet_mod.pth')}\")\n",
        "\n",
        "\n",
        "pytorch_model_references = {\n",
        "    \"ResNet50\": resnet_pytorch,\n",
        "    \"SqueezeNet\": sqnet_pytorch\n",
        "}\n",
        "\n",
        "# ─── LOAD ONNX MODELS (SESSIONS) ───────────────────────────────────────────\n",
        "onnx_model_file_paths = {\n",
        "    \"ResNet50_FP32\": os.path.join(DRIVE_MODEL_DIR, \"resnet50_fp32.onnx\"),\n",
        "    \"ResNet50_FP16\": os.path.join(DRIVE_MODEL_DIR, \"resnet50_fp16.onnx\"),\n",
        "    \"SqueezeNet_FP32\": os.path.join(DRIVE_MODEL_DIR, \"sqnet_fp32.onnx\"),\n",
        "    \"SqueezeNet_FP16\": os.path.join(DRIVE_MODEL_DIR, \"sqnet_fp16.onnx\"),\n",
        "}\n",
        "\n",
        "onnx_sessions = {}\n",
        "for name, path in onnx_model_file_paths.items():\n",
        "    if not os.path.exists(path):\n",
        "        log_inference(f\"ERROR: ONNX model path not found: {path}\")\n",
        "        continue\n",
        "    try:\n",
        "        providers = rt.get_available_providers()\n",
        "        preferred_providers = []\n",
        "        if 'CUDAExecutionProvider' in providers: preferred_providers.append('CUDAExecutionProvider')\n",
        "        if 'CPUExecutionProvider' in providers: preferred_providers.append('CPUExecutionProvider')\n",
        "        if not preferred_providers: raise RuntimeError(\"No suitable ONNX Execution Provider found.\")\n",
        "\n",
        "        onnx_sessions[name] = rt.InferenceSession(path, providers=preferred_providers)\n",
        "        log_inference(f\"ONNX session created for {name} using {onnx_sessions[name].get_providers()}\")\n",
        "    except Exception as e:\n",
        "        log_inference(f\"ERROR: Failed to create ONNX session for {name}. Error: {e}\")\n",
        "\n",
        "def infer_pytorch(model, image_tensor_batch):\n",
        "    with torch.no_grad():\n",
        "        logits = model(image_tensor_batch.to(DEVICE))\n",
        "\n",
        "        if isinstance(model, models.SqueezeNet) and logits.dim() > 2 :\n",
        "             logits = logits.view(logits.size(0), NUM_CLASSES)\n",
        "\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        predicted_class = torch.argmax(probabilities, dim=1)\n",
        "    return logits.cpu().numpy(), probabilities.cpu().numpy(), predicted_class.cpu().numpy()\n",
        "\n",
        "def infer_onnx(session, image_numpy_batch):\n",
        "    inp_meta = session.get_inputs()[0]\n",
        "    inp_name = inp_meta.name\n",
        "\n",
        "    elem_type_str = rt.OrtTensorType.elem_type_to_string(inp_meta.type.tensor_type.elem_type) if hasattr(inp_meta.type, 'tensor_type') else inp_meta.type\n",
        "    expects_fp16 = \"float16\" in elem_type_str.lower()\n",
        "\n",
        "    if expects_fp16 and image_numpy_batch.dtype != np.float16:\n",
        "        image_numpy_batch = image_numpy_batch.astype(np.float16)\n",
        "    elif not expects_fp16 and image_numpy_batch.dtype != np.float32:\n",
        "         image_numpy_batch = image_numpy_batch.astype(np.float32)\n",
        "\n",
        "    ort_inputs = {inp_name: image_numpy_batch}\n",
        "    logits = session.run(None, ort_inputs)[0]\n",
        "\n",
        "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "    probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "    predicted_class = np.argmax(probabilities, axis=1)\n",
        "    return logits, probabilities, predicted_class\n",
        "\n",
        "FIXED_SAMPLE_IMAGES_FILENAMES = {\n",
        "    \"0\": [\"aa048t2aaunaff005.png\", \"aa048t2aaunaff008.png\"],\n",
        "    \"1\": [\"aa048t2aeaff017.png\", \"aa048t2aeaff018.png\"],\n",
        "    \"2\": [\"ak064t1aaaff100.png\", \"ak064t1aaaff101.png\"],\n",
        "    \"3\": [\"ak064t1aaaff095.png\", \"ak064t1aaaff096.png\"]\n",
        "}\n",
        "\n",
        "\n",
        "log_inference(f\"\\nUsing FIXED sample images: {FIXED_SAMPLE_IMAGES_FILENAMES}\")\n",
        "collected_sample_image_paths = {}\n",
        "for class_name_str, filenames_in_class in FIXED_SAMPLE_IMAGES_FILENAMES.items():\n",
        "    current_class_paths = []\n",
        "    class_dir = os.path.join(DATA_ROOT, class_name_str)\n",
        "    if not os.path.isdir(class_dir):\n",
        "        log_inference(f\"Warning: Sample image class directory not found: {class_dir}. Skipping class {class_name_str}.\")\n",
        "        continue\n",
        "    for img_filename in filenames_in_class:\n",
        "        img_full_path = os.path.join(class_dir, img_filename)\n",
        "        if os.path.isfile(img_full_path):\n",
        "            current_class_paths.append(img_full_path)\n",
        "        else:\n",
        "            log_inference(f\"Warning: Specified sample image NOT FOUND: {img_full_path}. Skipping this image.\")\n",
        "    if current_class_paths:\n",
        "        collected_sample_image_paths[class_name_str] = current_class_paths\n",
        "\n",
        "if not collected_sample_image_paths:\n",
        "    log_inference(\"ERROR: No valid fixed sample images were found based on FIXED_SAMPLE_IMAGES_FILENAMES. Check paths and DATA_ROOT.\")\n",
        "else:\n",
        "    log_inference(f\"Verified sample images to be processed: {collected_sample_image_paths}\")\n",
        "\n",
        "# ─── PERFORM AND LOG INFERENCE ─────────────────────────────────────────────\n",
        "log_inference(\"\\n\" + \"=\"*30 + \" SAMPLE INFERENCE RESULTS \" + \"=\"*30)\n",
        "\n",
        "if not collected_sample_image_paths:\n",
        "    log_inference(\"Skipping inference as no sample images were loaded.\")\n",
        "else:\n",
        "    for class_name_str, image_paths_list in collected_sample_image_paths.items():\n",
        "        true_label_int = int(class_name_str)\n",
        "\n",
        "        for img_path in image_paths_list:\n",
        "            log_inference(f\"\\n--- Image: {img_path} (True Label: {true_label_int}) ---\")\n",
        "            try:\n",
        "                image_pil = Image.open(img_path).convert(\"RGB\")\n",
        "                image_tensor = val_tf(image_pil)\n",
        "                image_tensor_batch = image_tensor.unsqueeze(0)\n",
        "                image_numpy_batch = image_tensor_batch.cpu().numpy()\n",
        "\n",
        "                # --- PyTorch Inference ---\n",
        "                # ResNet50 PyTorch\n",
        "                pt_model_resnet = pytorch_model_references[\"ResNet50\"]\n",
        "                pt_logits, pt_probs, pt_pred = infer_pytorch(pt_model_resnet, image_tensor_batch)\n",
        "                log_inference(f\"  [PyTorch ResNet50]:\")\n",
        "                log_inference(f\"    Predicted Class: {pt_pred[0]}, Probabilities: {np.array2string(pt_probs[0], precision=4)}\")\n",
        "                log_inference(f\"    Logits: {np.array2string(pt_logits[0], precision=4)}\")\n",
        "\n",
        "                # SqueezeNet PyTorch\n",
        "                pt_model_sqnet = pytorch_model_references[\"SqueezeNet\"]\n",
        "                pt_logits, pt_probs, pt_pred = infer_pytorch(pt_model_sqnet, image_tensor_batch)\n",
        "                log_inference(f\"  [PyTorch SqueezeNet]:\")\n",
        "                log_inference(f\"    Predicted Class: {pt_pred[0]}, Probabilities: {np.array2string(pt_probs[0], precision=4)}\")\n",
        "                log_inference(f\"    Logits: {np.array2string(pt_logits[0], precision=4)}\")\n",
        "\n",
        "                # --- ONNX Inference ---\n",
        "                # ResNet50 ONNX models\n",
        "                if \"ResNet50_FP32\" in onnx_sessions:\n",
        "                    onnx_logits, onnx_probs, onnx_pred = infer_onnx(onnx_sessions[\"ResNet50_FP32\"], image_numpy_batch.copy())\n",
        "                    log_inference(f\"  [ONNX ResNet50_FP32]:\")\n",
        "                    log_inference(f\"    Predicted Class: {onnx_pred[0]}, Probabilities: {np.array2string(onnx_probs[0], precision=4)}\")\n",
        "                    log_inference(f\"    Logits: {np.array2string(onnx_logits[0], precision=4)}\")\n",
        "                if \"ResNet50_FP16\" in onnx_sessions:\n",
        "                    onnx_logits, onnx_probs, onnx_pred = infer_onnx(onnx_sessions[\"ResNet50_FP16\"], image_numpy_batch.copy())\n",
        "                    log_inference(f\"  [ONNX ResNet50_FP16]:\")\n",
        "                    log_inference(f\"    Predicted Class: {onnx_pred[0]}, Probabilities: {np.array2string(onnx_probs[0], precision=4)}\")\n",
        "                    log_inference(f\"    Logits: {np.array2string(onnx_logits[0], precision=4)}\")\n",
        "\n",
        "                # SqueezeNet ONNX models\n",
        "                if \"SqueezeNet_FP32\" in onnx_sessions:\n",
        "                    onnx_logits, onnx_probs, onnx_pred = infer_onnx(onnx_sessions[\"SqueezeNet_FP32\"], image_numpy_batch.copy())\n",
        "                    log_inference(f\"  [ONNX SqueezeNet_FP32]:\")\n",
        "                    log_inference(f\"    Predicted Class: {onnx_pred[0]}, Probabilities: {np.array2string(onnx_probs[0], precision=4)}\")\n",
        "                    log_inference(f\"    Logits: {np.array2string(onnx_logits[0], precision=4)}\")\n",
        "                if \"SqueezeNet_FP16\" in onnx_sessions:\n",
        "                    onnx_logits, onnx_probs, onnx_pred = infer_onnx(onnx_sessions[\"SqueezeNet_FP16\"], image_numpy_batch.copy())\n",
        "                    log_inference(f\"  [ONNX SqueezeNet_FP16]:\")\n",
        "                    log_inference(f\"    Predicted Class: {onnx_pred[0]}, Probabilities: {np.array2string(onnx_probs[0], precision=4)}\")\n",
        "                    log_inference(f\"    Logits: {np.array2string(onnx_logits[0], precision=4)}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                log_inference(f\"  ERROR processing image {img_path}: {e}\")\n",
        "                import traceback\n",
        "                log_inference(traceback.format_exc())\n",
        "\n",
        "log_inference(\"\\n\" + \"=\"*30 + \" Sample Inference Logging Complete \" + \"=\"*30 + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
